{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import imutils\n",
    "from skimage.measure import compare_ssim\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance as dist\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import copy\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=cv2.dnn.readNet(\"../Yolo/yolov3.weights\",\"../Yolo/yolov3.cfg\")\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL)\n",
    "layer_names = net.getLayerNames()\n",
    "outputlayers = [layer_names[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "with open(\"../Yolo/coco.names\",\"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert video to frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"./NMPS-CD/Road/Ref.mp4\")\n",
    "\n",
    "counter=1\n",
    "while (cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==0:\n",
    "        break\n",
    "    #cahnge path and file name\n",
    "    FrameNo = './NMPS-CD/Road/Ref/Frame'+str(counter)+'.jpg'\n",
    "    cv2.imwrite(FrameNo,frame)\n",
    "    counter = counter + 1\n",
    "cap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dir = '../NMPS-CD/Road/Ref'\n",
    "video_dir = '../NMPS-CD/Road/Track1'\n",
    "#video_dir = '../NMPS-CD/Road/Track2'\n",
    "ref_data = []\n",
    "video_data = []\n",
    "n_ref_frames=0\n",
    "n_video_frames=0\n",
    "i=1\n",
    "for f1 in os.listdir(ref_dir):\n",
    "    img = cv2.imread(os.path.join(ref_dir,'Frame'+str(i)+'.jpg'))\n",
    "    img = cv2.resize(img, (720,1280), interpolation = cv2.INTER_AREA)\n",
    "    ref_data.append(img)\n",
    "    n_ref_frames+=1\n",
    "    i+=1\n",
    "i=1\n",
    "for f1 in os.listdir(video_dir):\n",
    "    img = cv2.imread(os.path.join(video_dir,'Frame'+str(i)+'.jpg'))\n",
    "    img = cv2.resize(img, (720,1280), interpolation = cv2.INTER_AREA)\n",
    "    video_data.append(img)\n",
    "    n_video_frames+=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF_THRESHOLD = 0.7\n",
    "NMS_THRESHOLD = 0.4\n",
    "def post_process(frame, outs, conf_threshold, nms_threshold):\n",
    "    frame_height = frame.shape[0]\n",
    "    frame_width = frame.shape[1]\n",
    "\n",
    "    # Scan through all the bounding boxes output from the network and keep only\n",
    "    # the ones with high confidence scores. Assign the box's class label as the\n",
    "    # class with the highest score.\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    class_ids = []\n",
    "    final_boxes = []\n",
    "    final_classes=[]\n",
    "    final_confidences=[]\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > conf_threshold:\n",
    "                center_x = int(detection[0] * frame_width)\n",
    "                center_y = int(detection[1] * frame_height)\n",
    "                width = int(detection[2] * frame_width)\n",
    "                height = int(detection[3] * frame_height)\n",
    "                left = int(center_x - width / 2)\n",
    "                top = int(center_y - height / 2)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([left, top, width, height])\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Perform non maximum suppression to eliminate redundant\n",
    "    # overlapping boxes with lower confidences.\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold,\n",
    "                               nms_threshold)\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        left = box[0]\n",
    "        top = box[1]\n",
    "        width = box[2]\n",
    "        height = box[3]\n",
    "        final_boxes.append(box)\n",
    "        final_classes.append(class_ids[i])\n",
    "        final_confidences.append(confidences[i])\n",
    "    return zip(final_boxes, final_confidences, final_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 416\n",
    "IMG_HEIGHT = 416\n",
    "def yolo_get_objects(data):\n",
    "    objs = []\n",
    "    for image in data:\n",
    "        # Create a 4D blob from a frame.\n",
    "        blob = cv2.dnn.blobFromImage(image, 1 / 255, (IMG_WIDTH, IMG_HEIGHT),[0, 0, 0], 1, crop=False)\n",
    "\n",
    "        # Sets the input to the network\n",
    "        net.setInput(blob)\n",
    "\n",
    "        # Runs the forward pass to get output of the output layers\n",
    "        outs = net.forward(outputlayers)\n",
    "        \n",
    "        # Remove the bounding boxes with low confidence\n",
    "        objs.append(list(post_process(image, outs, CONF_THRESHOLD, NMS_THRESHOLD)))\n",
    "    return objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ref_objs = yolo_get_objects(ref_data)\n",
    "video_objs = yolo_get_objects(video_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--display]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\nanda\\AppData\\Roaming\\jupyter\\runtime\\kernel-2e2f320b-49af-4d66-8471-472170b85138.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nanda\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    SORT: A Simple, Online and Realtime Tracker\n",
    "    Copyright (C) 2016 Alex Bewley alex@dynamicdetection.com\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "from numba import jit\n",
    "import os.path\n",
    "import numpy as np\n",
    "##import matplotlib.pyplot as plt\n",
    "##import matplotlib.patches as patches\n",
    "from skimage import io\n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "import glob\n",
    "import time\n",
    "import argparse\n",
    "from filterpy.kalman import KalmanFilter\n",
    "\n",
    "@jit\n",
    "def iou(bb_test,bb_gt):\n",
    "  \"\"\"\n",
    "  Computes IUO between two bboxes in the form [x1,y1,x2,y2]\n",
    "  \"\"\"\n",
    "  xx1 = np.maximum(bb_test[0], bb_gt[0])\n",
    "  yy1 = np.maximum(bb_test[1], bb_gt[1])\n",
    "  xx2 = np.minimum(bb_test[2], bb_gt[2])\n",
    "  yy2 = np.minimum(bb_test[3], bb_gt[3])\n",
    "  w = np.maximum(0., xx2 - xx1)\n",
    "  h = np.maximum(0., yy2 - yy1)\n",
    "  wh = w * h\n",
    "  o = wh / ((bb_test[2]-bb_test[0])*(bb_test[3]-bb_test[1])\n",
    "    + (bb_gt[2]-bb_gt[0])*(bb_gt[3]-bb_gt[1]) - wh)\n",
    "  return(o)\n",
    "\n",
    "def convert_bbox_to_z(bbox):\n",
    "  \"\"\"\n",
    "  Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n",
    "    [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n",
    "    the aspect ratio\n",
    "  \"\"\"\n",
    "  w = bbox[2]-bbox[0]\n",
    "  h = bbox[3]-bbox[1]\n",
    "  x = bbox[0]+w/2.\n",
    "  y = bbox[1]+h/2.\n",
    "  s = w*h    #scale is just area\n",
    "  r = w/float(h)\n",
    "  return np.array([x,y,s,r]).reshape((4,1))\n",
    "\n",
    "def convert_x_to_bbox(x,score=None):\n",
    "  \"\"\"\n",
    "  Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\n",
    "    [x1,y1,x2,y2] where x1,y1 is the top left and x2,y2 is the bottom right\n",
    "  \"\"\"\n",
    "  w = np.sqrt(x[2]*x[3])\n",
    "  h = x[2]/w\n",
    "  if(score==None):\n",
    "    return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.]).reshape((1,4))\n",
    "  else:\n",
    "    return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.,score]).reshape((1,5))\n",
    "\n",
    "\n",
    "class KalmanBoxTracker(object):\n",
    "  \"\"\"\n",
    "  This class represents the internel state of individual tracked objects observed as bbox.\n",
    "  \"\"\"\n",
    "  count = 0\n",
    "  def __init__(self,bbox):\n",
    "    \"\"\"\n",
    "    Initialises a tracker using initial bounding box.\n",
    "    \"\"\"\n",
    "    #define constant velocity model\n",
    "    self.kf = KalmanFilter(dim_x=7, dim_z=4)\n",
    "    self.kf.F = np.array([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],  [0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]])\n",
    "    self.kf.H = np.array([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]])\n",
    "\n",
    "    self.kf.R[2:,2:] *= 10.\n",
    "    self.kf.P[4:,4:] *= 1000. #give high uncertainty to the unobservable initial velocities\n",
    "    self.kf.P *= 10.\n",
    "    self.kf.Q[-1,-1] *= 0.01\n",
    "    self.kf.Q[4:,4:] *= 0.01\n",
    "\n",
    "    self.kf.x[:4] = convert_bbox_to_z(bbox)\n",
    "    self.time_since_update = 0\n",
    "    self.id = KalmanBoxTracker.count\n",
    "    KalmanBoxTracker.count += 1\n",
    "    self.history = []\n",
    "    self.hits = 0\n",
    "    self.hit_streak = 0\n",
    "    self.age = 0\n",
    "    self.objclass = bbox[6]\n",
    "\n",
    "  def update(self,bbox):\n",
    "    \"\"\"\n",
    "    Updates the state vector with observed bbox.\n",
    "    \"\"\"\n",
    "    self.time_since_update = 0\n",
    "    self.history = []\n",
    "    self.hits += 1\n",
    "    self.hit_streak += 1\n",
    "    self.kf.update(convert_bbox_to_z(bbox))\n",
    "\n",
    "  def predict(self):\n",
    "    \"\"\"\n",
    "    Advances the state vector and returns the predicted bounding box estimate.\n",
    "    \"\"\"\n",
    "    if((self.kf.x[6]+self.kf.x[2])<=0):\n",
    "      self.kf.x[6] *= 0.0\n",
    "    self.kf.predict()\n",
    "    self.age += 1\n",
    "    if(self.time_since_update>0):\n",
    "      self.hit_streak = 0\n",
    "    self.time_since_update += 1\n",
    "    self.history.append(convert_x_to_bbox(self.kf.x))\n",
    "    return self.history[-1]\n",
    "\n",
    "  def get_state(self):\n",
    "    \"\"\"\n",
    "    Returns the current bounding box estimate.\n",
    "    \"\"\"\n",
    "    return convert_x_to_bbox(self.kf.x)\n",
    "\n",
    "def associate_detections_to_trackers(detections,trackers,iou_threshold = 0.3):\n",
    "  \"\"\"\n",
    "  Assigns detections to tracked object (both represented as bounding boxes)\n",
    "\n",
    "  Returns 3 lists of matches, unmatched_detections and unmatched_trackers\n",
    "  \"\"\"\n",
    "  if(len(trackers)==0):\n",
    "    return np.empty((0,2),dtype=int), np.arange(len(detections)), np.empty((0,5),dtype=int)\n",
    "  iou_matrix = np.zeros((len(detections),len(trackers)),dtype=np.float32)\n",
    "\n",
    "  for d,det in enumerate(detections):\n",
    "    for t,trk in enumerate(trackers):\n",
    "      iou_matrix[d,t] = iou(det,trk)\n",
    "  matched_indices = linear_assignment(-iou_matrix)\n",
    "\n",
    "  unmatched_detections = []\n",
    "  for d,det in enumerate(detections):\n",
    "    if(d not in matched_indices[:,0]):\n",
    "      unmatched_detections.append(d)\n",
    "  unmatched_trackers = []\n",
    "  for t,trk in enumerate(trackers):\n",
    "    if(t not in matched_indices[:,1]):\n",
    "      unmatched_trackers.append(t)\n",
    "\n",
    "  #filter out matched with low IOU\n",
    "  matches = []\n",
    "  for m in matched_indices:\n",
    "    if(iou_matrix[m[0],m[1]]<iou_threshold):\n",
    "      unmatched_detections.append(m[0])\n",
    "      unmatched_trackers.append(m[1])\n",
    "    else:\n",
    "      matches.append(m.reshape(1,2))\n",
    "  if(len(matches)==0):\n",
    "    matches = np.empty((0,2),dtype=int)\n",
    "  else:\n",
    "    matches = np.concatenate(matches,axis=0)\n",
    "\n",
    "  return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n",
    "\n",
    "\n",
    "\n",
    "class Sort(object):\n",
    "  def __init__(self,max_age=1,min_hits=3):\n",
    "    \"\"\"\n",
    "    Sets key parameters for SORT\n",
    "    \"\"\"\n",
    "    self.max_age = max_age\n",
    "    self.min_hits = min_hits\n",
    "    self.trackers = []\n",
    "    self.frame_count = 0\n",
    "    self.disappeared = OrderedDict()\n",
    "    self.maxDisappeared = maxDisappeared\n",
    "    self.appearing = OrderedDict()\n",
    "    self.lifetime = []\n",
    "\n",
    "    \n",
    "  def update(self,dets):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "      dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\n",
    "    Requires: this method must be called once for each frame even with empty detections.\n",
    "    Returns the a similar array, where the last column is the object ID.\n",
    "\n",
    "    NOTE: The number of objects returned may differ from the number of detections provided.\n",
    "    \"\"\"\n",
    "    self.frame_count += 1\n",
    "    #get predicted locations from existing trackers.\n",
    "    trks = np.zeros((len(self.trackers),5))\n",
    "    to_del = []\n",
    "    ret = []\n",
    "    for t,trk in enumerate(trks):\n",
    "      pos = self.trackers[t].predict()[0]\n",
    "      trk[:] = [pos[0], pos[1], pos[2], pos[3], 0]\n",
    "      if(np.any(np.isnan(pos))):\n",
    "        to_del.append(t)\n",
    "    trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n",
    "    for t in reversed(to_del):\n",
    "      self.trackers.pop(t)\n",
    "    matched, unmatched_dets, unmatched_trks = associate_detections_to_trackers(dets,trks)\n",
    "\n",
    "    #update matched trackers with assigned detections\n",
    "    for t,trk in enumerate(self.trackers):\n",
    "      if(t not in unmatched_trks):\n",
    "        d = matched[np.where(matched[:,1]==t)[0],0]\n",
    "        trk.update(dets[d,:][0])\n",
    "\n",
    "    #create and initialise new trackers for unmatched detections\n",
    "    for i in unmatched_dets:\n",
    "        trk = KalmanBoxTracker(dets[i,:])\n",
    "        self.trackers.append(trk)\n",
    "    i = len(self.trackers)\n",
    "    for trk in reversed(self.trackers):\n",
    "        d = trk.get_state()[0]\n",
    "        if((trk.time_since_update < 1) and (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits)):\n",
    "          ret.append(np.concatenate((d,[trk.id+1], [trk.objclass])).reshape(1,-1)) # +1 as MOT benchmark requires positive\n",
    "        i -= 1\n",
    "        #remove dead tracklet\n",
    "        if(trk.time_since_update > self.max_age):\n",
    "          self.trackers.pop(i)\n",
    "    if(len(ret)>0):\n",
    "      return np.concatenate(ret)\n",
    "    return np.empty((0,5))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ref_ct = Sort()\n",
    "ref_obj_track = []\n",
    "\n",
    "for i in range(0,len(ref_data)):\n",
    "    objects = ref_ct.update(torch.tensor([[box[0],box[1],box[2]+box[0],box[3]+box[1],conf,0.9,cls] for box,conf,cls in ref_objs[i]]))\n",
    "    ref_obj_track.append(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ct = Sort()\n",
    "video_obj_track = []\n",
    "\n",
    "for i in range(0,len(video_data)):\n",
    "    objects = video_ct.update(torch.tensor([[box[0],box[1],box[2]+box[0],box[3]+box[1],conf,0.9,cls] for box,conf,cls in video_objs[i]]))\n",
    "    video_obj_track.append(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0000e+00,  4.8000e+02,  7.0700e+02,  9.4100e+02,  9.9899e-01,\n",
       "          9.0000e-01,  2.0000e+00]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[box[0],box[1],box[2]+box[0],box[3]+box[1],conf,0.9,cls] for box,conf,cls in ref_objs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ct = Sort()\n",
    "video_obj_track = []\n",
    "for i in range(0,len(video_data)):\n",
    "    if video_data[i] is not None:\n",
    "        output_frames = copy.deepcopy(video_data[i])\n",
    "        objects = video_ct.update(torch.tensor([[box[0],box[1],box[2]+box[0],box[3]+box[1],conf,0.9,cls] for box,conf,cls in video_objs[i]]))\n",
    "        for x1, y1, x2, y2, obj_id, cls_pred in objects:\n",
    "            cv2.rectangle(output_frames, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 4)\n",
    "            cv2.putText(output_frames, classes[int(cls_pred)] + \"-\" + str(int(obj_id)), (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 3)\n",
    "        ref_obj_track.append(objects)\n",
    "        cv2.namedWindow(\"output\", cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(\"output\", output_frames)\n",
    "    \n",
    "        time.sleep(0.05)\n",
    "    \n",
    "        keyboard = cv2.waitKey(30) & 0xFF\n",
    "        if keyboard == 'q' or keyboard == 27:\n",
    "            break\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(ref_data)):\n",
    "    if ref_data[i] is not None:\n",
    "        \n",
    "        cv2.namedWindow(\"output\", cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(\"output\", ref_data[i])\n",
    "    \n",
    "        #time.sleep(0.1)\n",
    "    \n",
    "        keyboard = cv2.waitKey(30) & 0xFF\n",
    "        if keyboard == 'q' or keyboard == 27:\n",
    "            break\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"output\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"output\", video_data[0])\n",
    "keyboard = cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pc-python",
   "language": "python",
   "name": "pc-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
