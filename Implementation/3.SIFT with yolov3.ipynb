{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python==3.4.2.16\n",
    "!pip install imutils\n",
    "!pip install scikit-image\n",
    "!pip install matplotlib\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import imutils\n",
    "from skimage.measure import compare_ssim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=cv2.dnn.readNet(\"../Yolo/yolov3.weights\",\"../Yolo/yolov3.cfg\")\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL)\n",
    "layer_names = net.getLayerNames()\n",
    "outputlayers = [layer_names[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "with open(\"../Yolo/coco.names\",\"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert video to frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change video name\n",
    "cap = cv2.VideoCapture(\"./NMPS-CD/Road/Ref.mp4\")\n",
    "\n",
    "counter=1\n",
    "while (cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==0:\n",
    "        break\n",
    "    #cahnge path and file name\n",
    "    FrameNo = './NMPS-CD/Road/Ref/Frame'+str(counter)+'.jpg'\n",
    "    cv2.imwrite(FrameNo,frame)\n",
    "    counter = counter + 1\n",
    "cap.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dir = '../NMPS-CD/Road/Ref'\n",
    "video_dir = '../NMPS-CD/Road/Track1'\n",
    "#video_dir = '../NMPS-CD/Road/Track2'\n",
    "ref_data = []\n",
    "video_data = []\n",
    "n_ref_frames=0\n",
    "n_video_frames=0\n",
    "i=1\n",
    "for f1 in os.listdir(ref_dir):\n",
    "    img = cv2.imread(os.path.join(ref_dir,'Frame'+str(i)+'.jpg'))\n",
    "    img = cv2.resize(img, (720,1280), interpolation = cv2.INTER_AREA)\n",
    "    ref_data.append(img)\n",
    "    n_ref_frames+=1\n",
    "    i+=1\n",
    "i=1\n",
    "for f1 in os.listdir(video_dir):\n",
    "    img = cv2.imread(os.path.join(video_dir,'Frame'+str(i)+'.jpg'))\n",
    "    img = cv2.resize(img, (720,1280), interpolation = cv2.INTER_AREA)\n",
    "    video_data.append(img)\n",
    "    n_video_frames+=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find objects in each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF_THRESHOLD = 0.5\n",
    "NMS_THRESHOLD = 0.4\n",
    "def post_process(frame, outs, conf_threshold, nms_threshold):\n",
    "    frame_height = frame.shape[0]\n",
    "    frame_width = frame.shape[1]\n",
    "\n",
    "    # Scan through all the bounding boxes output from the network and keep only\n",
    "    # the ones with high confidence scores. Assign the box's class label as the\n",
    "    # class with the highest score.\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    class_ids = []\n",
    "    final_boxes = []\n",
    "    final_confidence = []\n",
    "    final_classes=[]\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > conf_threshold:\n",
    "                center_x = int(detection[0] * frame_width)\n",
    "                center_y = int(detection[1] * frame_height)\n",
    "                width = int(detection[2] * frame_width)\n",
    "                height = int(detection[3] * frame_height)\n",
    "                left = int(center_x - width / 2)\n",
    "                top = int(center_y - height / 2)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([left, top, width, height])\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Perform non maximum suppression to eliminate redundant\n",
    "    # overlapping boxes with lower confidences.\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold,\n",
    "                               nms_threshold)\n",
    "\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        left = box[0]\n",
    "        top = box[1]\n",
    "        width = box[2]\n",
    "        height = box[3]\n",
    "        final_boxes.append(box)\n",
    "        final_classes.append(str(classes[class_ids[i]]))\n",
    "        #left, top, right, bottom = refined_box(left, top, width, height)\n",
    "        #draw_predict(frame, confidences[i], left, top, left + width,\n",
    "        #              top + height)\n",
    "        #draw_predict(frame, confidences[i], left, top, right, bottom)\n",
    "    return zip(final_boxes,  final_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 416\n",
    "IMG_HEIGHT = 416\n",
    "def yolo_get_objects(data):\n",
    "    objs = []\n",
    "    for image in data:\n",
    "        # Create a 4D blob from a frame.\n",
    "        blob = cv2.dnn.blobFromImage(image, 1 / 255, (IMG_WIDTH, IMG_HEIGHT),[0, 0, 0], 1, crop=False)\n",
    "\n",
    "        # Sets the input to the network\n",
    "        net.setInput(blob)\n",
    "\n",
    "        # Runs the forward pass to get output of the output layers\n",
    "        outs = net.forward(outputlayers)\n",
    "\n",
    "        # Remove the bounding boxes with low confidence\n",
    "        objs.append(list(post_process(image, outs, CONF_THRESHOLD, NMS_THRESHOLD)))\n",
    "    return objs\n",
    "\n",
    "ref_objs = yolo_get_objects(ref_data)\n",
    "video_objs = yolo_get_objects(video_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIFT and change detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh):\n",
    "    # convert the keypoints to numpy arrays\n",
    "    kpsA = np.float32([kp.pt for kp in kpsA])\n",
    "    kpsB = np.float32([kp.pt for kp in kpsB])\n",
    "    \n",
    "    if len(matches) > 4:\n",
    "\n",
    "        # construct the two sets of points\n",
    "        ptsA = np.float32([kpsA[m.queryIdx] for m in matches])\n",
    "        ptsB = np.float32([kpsB[m.trainIdx] for m in matches])\n",
    "        \n",
    "        # estimate the homography between the sets of points\n",
    "        (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,\n",
    "            reprojThresh)\n",
    "        return (matches, H, status)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_similar_objs(img1, img2, list1, list2):\n",
    "    width = img1.shape[0]\n",
    "    height = img1.shape[1]\n",
    "    threshold = 10\n",
    "    if len(list1) == 0 or len(list2) == 0:\n",
    "        return (list1, list2)\n",
    "    for obj1 in list1:\n",
    "        left1, top1, width1, height1 = obj1[0]\n",
    "        obj1_img = img1\n",
    "        mask = np.zeros((width, height),np.uint8)\n",
    "        mask = cv2.rectangle(mask, (left1, top1), (left1+width1, top1+height1), 1, thickness=-1)\n",
    "        obj1_masked = cv2.bitwise_and(obj1_img, obj1_img, mask=mask)\n",
    "        \n",
    "        for obj2 in list2:\n",
    "            left2, top2, width2, height2 = obj2[0]\n",
    "            obj2_img = img2\n",
    "            mask = np.zeros((width, height),np.uint8)\n",
    "            mask = cv2.rectangle(mask, (left2, top2), (left2+width2, top2+height2), 1, thickness=-1)\n",
    "            obj2_masked = cv2.bitwise_and(obj2_img, obj2_img, mask=mask)\n",
    "            \n",
    "            orb_detector = cv2.ORB_create(5000)\n",
    "            kp1, d1 = orb_detector.detectAndCompute(obj1_masked, None) \n",
    "            kp2, d2 = orb_detector.detectAndCompute(obj2_masked, None)\n",
    "            matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "            matches = matcher.match(d1, d2) \n",
    "            \n",
    "            if(len(matches) >= threshold):\n",
    "                list1.remove(obj1)\n",
    "                list2.remove(obj2)\n",
    "    return (list1, list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_objs(img1, img2, list1, list2):\n",
    "    width = img1.shape[0]\n",
    "    height = img1.shape[1]\n",
    "    threshold = 20\n",
    "    img1_list = []\n",
    "    img2_list = []\n",
    "    if len(list1) == 0 or len(list2) == 0:\n",
    "        return (list1, list2)\n",
    "    for obj1 in list1:\n",
    "        left1, top1, width1, height1 = obj1[0]\n",
    "        obj1_img = img1\n",
    "        mask = np.zeros((width, height),np.uint8)\n",
    "        mask = cv2.rectangle(mask, (left1, top1), (left1+width1, top1+height1), 1, thickness=-1)\n",
    "        obj1_masked = cv2.bitwise_and(obj1_img, obj1_img, mask=mask)\n",
    "        \n",
    "        for obj2 in list2:\n",
    "            left2, top2, width2, height2 = obj2[0]\n",
    "            obj2_img = img2\n",
    "            mask = np.zeros((width, height),np.uint8)\n",
    "            mask = cv2.rectangle(mask, (left2, top2), (left2+width2, top2+height2), 1, thickness=-1)\n",
    "            obj2_masked = cv2.bitwise_and(obj2_img, obj2_img, mask=mask)\n",
    "            \n",
    "            orb_detector = cv2.ORB_create(5000)\n",
    "            kp1, d1 = orb_detector.detectAndCompute(obj1_masked, None) \n",
    "            kp2, d2 = orb_detector.detectAndCompute(obj2_masked, None)\n",
    "            matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "            matches = matcher.match(d1, d2) \n",
    "            \n",
    "            if(len(matches) >= threshold):\n",
    "                img1_list.append(obj1)\n",
    "                img2_list.append(obj2)\n",
    "    return (img1_list, img1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_obj_mask(shape, list):\n",
    "    mask = np.ones(shape,np.uint8)\n",
    "    for obj,label in list:\n",
    "        left, top, width, height = obj\n",
    "        mask = cv2.rectangle(mask, (left, top), (left+width, top+height), 0, thickness=-1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inverted_obj_mask(shape, list):\n",
    "    mask = np.ones(shape,np.uint8)\n",
    "    for obj,label in list:\n",
    "        left, top, width, height = obj\n",
    "        mask = cv2.rectangle(mask, (left, top), (left+width, top+height), 0, thickness=-1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_ptr = 0\n",
    "for i in range(0,n_video_frames):\n",
    "    # Open the image files. \n",
    "    img1_color = video_data[i]  # Image to be aligned. \n",
    "    img2_color = None    # Reference image. \n",
    "    matches=0\n",
    "    kp1,d1=None,None\n",
    "    kp2,d2=None,None\n",
    "    while(True):\n",
    "        img2_color=ref_data[ref_ptr]\n",
    "        img1 = cv2.cvtColor(img1_color, cv2.COLOR_BGR2GRAY) \n",
    "        img2 = cv2.cvtColor(img2_color, cv2.COLOR_BGR2GRAY)\n",
    "        img2_prev = None\n",
    "        img2_fwd = None\n",
    "        img1_prev_objs, img2_prev_objs = None, None\n",
    "        img1_fwd_objs, img2_fwd_objs = None, None\n",
    "        width, height = img2.shape\n",
    "        ref_prev_ptr = ref_ptr\n",
    "        ref_fwd_ptr = ref_ptr\n",
    "        if(ref_ptr>1):\n",
    "            ref_prev_ptr = ref_ptr-1\n",
    "        if(ref_ptr<len(ref_data)-1):\n",
    "            ref_fwd_ptr = ref_ptr+1\n",
    "        \n",
    "        \n",
    "        img2_prev = cv2.cvtColor(ref_data[ref_prev_ptr], cv2.COLOR_BGR2GRAY)\n",
    "        img2_fwd = cv2.cvtColor(ref_data[ref_fwd_ptr], cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        img1_objs, img2_objs = remove_similar_objs(img1, img2, video_objs[i].copy(), ref_objs[ref_ptr].copy())\n",
    "        img1_prev_objs, img2_prev_objs = remove_similar_objs(img1, img2_prev, video_objs[i].copy(), ref_objs[ref_prev_ptr].copy())\n",
    "        img1_fwd_objs, img2_fwd_objs = remove_similar_objs(img1, img2_fwd, video_objs[i].copy(), ref_objs[ref_fwd_ptr].copy())\n",
    "        \n",
    "        orb_detector = cv2.ORB_create(5000) \n",
    "        orb_detector_prev = cv2.ORB_create(5000)\n",
    "        orb_detector_fwd = cv2.ORB_create(5000)\n",
    "\n",
    "        kp1, d1 = orb_detector.detectAndCompute(img1, generate_obj_mask((width,height), img1_objs)) \n",
    "        kp2, d2 = orb_detector.detectAndCompute(img2, generate_obj_mask((width,height), img2_objs)) \n",
    "        kp3, d3 = orb_detector_prev.detectAndCompute(img1, generate_obj_mask((width,height), img1_prev_objs))\n",
    "        kp4, d4 = orb_detector_prev.detectAndCompute(img2_prev, generate_obj_mask((width,height), img2_prev_objs))\n",
    "        kp5, d5 = orb_detector.detectAndCompute(img1, generate_obj_mask((width,height), img1_fwd_objs)) \n",
    "        kp6, d6 = orb_detector.detectAndCompute(img2_fwd, generate_obj_mask((width,height), img2_fwd_objs))\n",
    " \n",
    "        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True) \n",
    "\n",
    "        matches = matcher.match(d1, d2) \n",
    "        matches_prev = matcher.match(d3, d4) \n",
    "        matches_fwd=matcher.match(d5, d6) \n",
    "        \n",
    "        if(max(len(matches),len(matches_prev),len(matches_fwd)) == len(matches)):\n",
    "            break\n",
    "        elif(max(len(matches),len(matches_prev),len(matches_fwd)) == len(matches_prev)):\n",
    "            ref_ptr-=2\n",
    "            continue\n",
    "        else:\n",
    "            ref_ptr+=2\n",
    "            continue\n",
    "\n",
    "    matches.sort(key = lambda x: x.distance) \n",
    " \n",
    "    matches = matches[:int(len(matches)*90)] \n",
    "    no_of_matches = len(matches) \n",
    "\n",
    "    p1 = np.zeros((no_of_matches, 2)) \n",
    "    p2 = np.zeros((no_of_matches, 2)) \n",
    "\n",
    "    for i in range(len(matches)): \n",
    "        p1[i, :] = kp1[matches[i].queryIdx].pt \n",
    "        p2[i, :] = kp2[matches[i].trainIdx].pt \n",
    "\n",
    "    homography, mask = cv2.findHomography(p1, p2, cv2.RANSAC) \n",
    "\n",
    "    transformed_img = cv2.warpPerspective(img1_color, \n",
    "                        homography, (height, width)) \n",
    "\n",
    "    (score, diff) = compare_ssim(img2,cv2.cvtColor(transformed_img, cv2.COLOR_BGR2GRAY), full=True)\n",
    "    diff = (diff * 255).astype(\"uint8\")\n",
    "                                                             \n",
    "    thresh = cv2.threshold(diff, 0, 255,cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    result_img = np.concatenate((np.concatenate((img1,img2),axis=1), np.concatenate((diff,thresh),axis=1)), axis=0)\n",
    "                                                             \n",
    "    cv2.namedWindow(\"output\", cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(\"output\", result_img)\n",
    "    cv2.imwrite('../NMPS-CD/Road/Output-Track1/'+str(i)+'.jpg',result_img)\n",
    "    \n",
    "    keyboard = cv2.waitKey(30) & 0xFF\n",
    "    if keyboard == 'q' or keyboard == 27:\n",
    "        break\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"thresh\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"thresh\", ref_data[0])\n",
    "keyboard = cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = genereate_obj_mask((1280,720),)\n",
    "img1 = cv2.cvtColor(video_data[0], cv2.COLOR_BGR2GRAY) \n",
    "obj1_masked = cv2.bitwise_and(img1, img1, mask=mask)\n",
    "cv2.namedWindow(\"mask1\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"mask1\", obj1_masked)\n",
    "cv2.namedWindow(\"mask\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"mask\", mask)\n",
    "keyboard = cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_new_objs(frames, objs):\n",
    "    new_objs = []\n",
    "    history = []\n",
    "    for i in range(1,len(frames),1):\n",
    "        img1_objs, img2_objs = remove_similar_objs(frames[i-1], frames[i], objs[i-1].copy(), objs[i].copy())\n",
    "        for obj in img2_objs:\n",
    "            new_objs.append((i,obj))\n",
    "    return new_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_new_objs = find_new_objs(ref_data, ref_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_new_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for frame in ref_data:\n",
    "    cv2.namedWindow(\"mask\", cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(\"mask\", frame)\n",
    "    time.sleep(0.3)\n",
    "    keyboard = cv2.waitKey(30) & 0xFF\n",
    "    if keyboard == 'q' or keyboard == 27:\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"mask\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"mask\", ref_data[84])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "class CentroidTracker():\n",
    "    def __init__(self, maxDisappeared=30):\n",
    "        self.nextObjectID = 0\n",
    "        self.objects = OrderedDict()\n",
    "        self.disappeared = OrderedDict()\n",
    "        self.maxDisappeared = maxDisappeared\n",
    "        self.appearing = []\n",
    "        self.disappearing = []\n",
    "        self.frameNo = 0\n",
    "\n",
    "    def register(self, centroid):\n",
    "        self.objects[self.nextObjectID] = centroid\n",
    "        self.disappeared[self.nextObjectID] = 0\n",
    "        self.appearing.append((frameNo, self.nextObjectID, centroid))\n",
    "        self.nextObjectID += 1\n",
    "\n",
    "    def deregister(self, objectID):\n",
    "        self.disappearing.append((frameNo, self.nextObjectID,self.objects[objectID]))\n",
    "        del self.objects[objectID]\n",
    "        del self.disappeared[objectID]\n",
    "\n",
    "    def update(self, rects):\n",
    "        if len(rects) == 0:\n",
    "            for objectID in list(self.disappeared.keys()):\n",
    "                self.disappeared[objectID] += 1\n",
    "                if self.disappeared[objectID] > self.maxDisappeared:\n",
    "                    self.deregister(objectID)\n",
    "            return self.objects\n",
    "        inputCentroids = np.zeros((len(rects), 2), dtype=\"int\")\n",
    "        for (i, (startX, startY, endX, endY)) in enumerate(rects):\n",
    "            cX = int((2*startX + endX) / 2.0)\n",
    "            cY = int((2*startY + endY) / 2.0)\n",
    "            inputCentroids[i] = (cX, cY)\n",
    "        if len(self.objects) == 0:\n",
    "            for i in range(0, len(inputCentroids)):\n",
    "                self.register(inputCentroids[i])\n",
    "        else:\n",
    "            objectIDs = list(self.objects.keys())\n",
    "            objectCentroids = list(self.objects.values())\n",
    "            D = dist.cdist(np.array(objectCentroids), inputCentroids)\n",
    "            rows = D.min(axis=1).argsort()\n",
    "            cols = D.argmin(axis=1)[rows]\n",
    "            usedRows = set()\n",
    "            usedCols = set()\n",
    "            for (row, col) in zip(rows, cols):\n",
    "                if row in usedRows or col in usedCols:\n",
    "                    continue\n",
    "                objectID = objectIDs[row]\n",
    "                self.objects[objectID] = inputCentroids[col]\n",
    "                self.disappeared[objectID] = 0\n",
    "                usedRows.add(row)\n",
    "                usedCols.add(col)\n",
    "            unusedRows = set(range(0, D.shape[0])).difference(usedRows)\n",
    "            unusedCols = set(range(0, D.shape[1])).difference(usedCols)\n",
    "            if D.shape[0] >= D.shape[1]:\n",
    "                for row in unusedRows:\n",
    "                    objectID = objectIDs[row]\n",
    "                    self.disappeared[objectID] += 1\n",
    "                    if self.disappeared[objectID] > self.maxDisappeared:\n",
    "                        self.deregister(objectID)\n",
    "            else:\n",
    "                for col in unusedCols:\n",
    "                    self.register(inputCentroids[col])\n",
    "        frameNo+=1\n",
    "        return self.objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CentroidTracker()\n",
    "(H, W) = (None, None)\n",
    "\n",
    "for i in range(0,len(ref_data)):\n",
    "    objects = ct.update([box[0] for box in ref_objs[i]])\n",
    "    #frame = ref_data[i].copy()\n",
    "    #for obj in ref_objs[i]:\n",
    "    #    left, top, width, height = obj[0]\n",
    "    #    cv2.rectangle(frame, (left, top), (left + width,top + height),(0, 255, 0),2)\n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "    # loop over the tracked objects\n",
    "    for (objectID, centroid) in objects.items():\n",
    "        # draw both the ID of the object and the centroid of the\n",
    "        # object on the output frame\n",
    "        text = \"ID {}\".format(objectID)\n",
    "        cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.namedWindow(\"Frame\", cv2.WINDOW_NORMAL);cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_objs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import compare_ssim as ssim\n",
    "def mse(imageA, imageB):\n",
    "    err = np.sum((imageA.astype(\"float\")-imageB.astype(\"float\"))**2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return err\n",
    "def compare_image(imageA,imageB):\n",
    "    m = mse(imageA,imageB)\n",
    "    s = ssim(imageA,imageB)\n",
    "    fig = plt.figure(\"abc\")\n",
    "    plt.suptitle(\"MSE: %.2f, SSIM: %.2f\" % (m,s))\n",
    "    \n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    plt.imshow(imageA, cmap = plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    plt.imshow(imageB, cmap = plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    "    return (mse, ssim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
