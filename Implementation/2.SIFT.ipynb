{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python==3.4.2.16\n",
    "!pip install imutils\n",
    "!pip install skimage\n",
    "!pip install matplotlib\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from skimage.measure import compare_ssim\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./NMPS-CD/Road/Ref/Frame1.jpg')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dir = '../NMPS-CD/Road/Ref'\n",
    "video_dir = '../NMPS-CD/Road/Track1'\n",
    "#video_dir = '../NMPS-CD/Road/Track2'\n",
    "ref_data = []\n",
    "video_data = []\n",
    "n_ref_frames=0\n",
    "n_video_frames=0\n",
    "i=1\n",
    "for f1 in os.listdir(ref_dir):\n",
    "    img = cv2.imread(os.path.join(ref_dir,'Frame'+str(i)+'.jpg'))\n",
    "    img = cv2.resize(img, (720,1280), interpolation = cv2.INTER_AREA)\n",
    "    ref_data.append(img)\n",
    "    n_ref_frames+=1\n",
    "    i+=1\n",
    "i=1\n",
    "for f1 in os.listdir(video_dir):\n",
    "    img = cv2.imread(os.path.join(video_dir,'Frame'+str(i)+'.jpg'))\n",
    "    img = cv2.resize(img, (720,1280), interpolation = cv2.INTER_AREA)\n",
    "    video_data.append(img)\n",
    "    n_video_frames+=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh):\n",
    "    # convert the keypoints to numpy arrays\n",
    "    kpsA = np.float32([kp.pt for kp in kpsA])\n",
    "    kpsB = np.float32([kp.pt for kp in kpsB])\n",
    "    \n",
    "    if len(matches) > 4:\n",
    "\n",
    "        # construct the two sets of points\n",
    "        ptsA = np.float32([kpsA[m.queryIdx] for m in matches])\n",
    "        ptsB = np.float32([kpsB[m.trainIdx] for m in matches])\n",
    "        \n",
    "        # estimate the homography between the sets of points\n",
    "        (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,\n",
    "            reprojThresh)\n",
    "        return (matches, H, status)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "trainImg=cv2.imread('./NMPS-CD/Road/Ref/Frame1.jpg')\n",
    "trainImg_gray=cv2.imread('./NMPS-CD/Road/Ref/Frame1.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "queryImg=cv2.imread('./NMPS-CD/Road/Track1/Frame10.jpg')\n",
    "queryImg_gray=cv2.imread('./NMPS-CD/Road/Track1/Frame10.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#Detect keypoints and compute descriptors using AKAZE\n",
    "descriptor = cv2.ORB_create()\n",
    "descriptor = cv2.AKAZE_create()\n",
    "kpsA, featuresA = descriptor.detectAndCompute(trainImg_gray, None)\n",
    "kpsB, featuresB = descriptor.detectAndCompute(queryImg_gray, None)\n",
    "\n",
    "#Use brute-force matcher to find 2-nn matches\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "nn_matches = bf.knnMatch(featuresA, featuresB, 2)\n",
    "\n",
    "# loop over the raw matches\n",
    "matches = []\n",
    "ratio=0.4\n",
    "for m, n in nn_matches:\n",
    "    if m.distance < ratio * n.distance:\n",
    "        matches.append(m)\n",
    "img3 = cv2.drawMatches(trainImg,kpsA,queryImg,kpsB,np.random.choice(matches,100),\n",
    "                           None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img3)\n",
    "plt.show()\n",
    "\n",
    "cv2.namedWindow(\"img\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"img\",cv2.resize(img3, (960, 540)))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh=4)\n",
    "if M is None:\n",
    "    print(\"Error!\")\n",
    "(matches, H, status) = M\n",
    "print(H)\n",
    "print(matches[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply panorama correction\n",
    "width = trainImg.shape[1] + queryImg.shape[1]\n",
    "height = trainImg.shape[0] + queryImg.shape[0]\n",
    "\n",
    "result = cv2.warpPerspective(trainImg, H, (width, height))\n",
    "#result[0:(queryImg.shape[0]+0), 0:(queryImg.shape[1]+0)] = queryImg\n",
    "(score, diff) = compare_ssim(trainImg_gray,cv2.cvtColor(result[0:(queryImg.shape[0]+0), 0:(queryImg.shape[1]+0)], cv2.COLOR_BGR2GRAY),full=True)\n",
    "diff = (diff * 255).astype(\"uint8\")\n",
    "thresh = cv2.threshold(diff, 0, 255,cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "cv2.namedWindow(\"thresh\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"thresh\", thresh)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(result)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np \n",
    "\n",
    "#out = cv2.VideoWriter('outpy.avi',cv2.VideoWriter_fourcc(*\"MJPG\"), 30.0, (1280,720))\n",
    "#blurw = cv2.VideoWriter('outpy_blur,.avi',cv2.VideoWriter_fourcc(*\"MJPG\"), 30.0, (1280,720))\n",
    "ref_ptr=0\n",
    "\n",
    "for i in range(0,n_video_frames):\n",
    "    # Open the image files. \n",
    "    img1_color = video_data[i]  # Image to be aligned. \n",
    "    img2_color = None    # Reference image. \n",
    "    matches=0\n",
    "    kp1,d1=None,None\n",
    "    kp2,d2=None,None\n",
    "    while(True):\n",
    "    # Convert to grayscale. \n",
    "        img2_color=ref_data[ref_ptr]\n",
    "        img1 = cv2.cvtColor(img1_color, cv2.COLOR_BGR2GRAY) \n",
    "        img2 = cv2.cvtColor(img2_color, cv2.COLOR_BGR2GRAY) \n",
    "        img2_prev = None\n",
    "        img2_fwd = None\n",
    "        if(ref_ptr>5):\n",
    "            img2_prev = cv2.cvtColor(ref_data[ref_ptr-5], cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            img2_prev = cv2.cvtColor(ref_data[ref_ptr], cv2.COLOR_BGR2GRAY)\n",
    "        if(ref_ptr<len(ref_data)-5):\n",
    "            img2_fwd = cv2.cvtColor(ref_data[ref_ptr+5], cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            img2_fwd = cv2.cvtColor(ref_data[ref_ptr], cv2.COLOR_BGR2GRAY)\n",
    "        height, width = img2.shape \n",
    "\n",
    "        # Create ORB detector with 5000 features. \n",
    "        orb_detector = cv2.ORB_create(5000) \n",
    "        orb_detector_prev = cv2.ORB_create(5000)\n",
    "        orb_detector_fwd = cv2.ORB_create(5000)\n",
    "        # Find keypoints and descriptors. \n",
    "        # The first arg is the image, second arg is the mask \n",
    "        #  (which is not reqiured in this case). \n",
    "        kp1, d1 = orb_detector.detectAndCompute(img1, None) \n",
    "        kp2, d2 = orb_detector.detectAndCompute(img2, None) \n",
    "        \n",
    "        kp3, d3 = orb_detector_prev.detectAndCompute(img1, None) \n",
    "        kp4, d4 = orb_detector_prev.detectAndCompute(img2_prev, None)\n",
    "        \n",
    "        kp5, d5 = orb_detector.detectAndCompute(img1, None) \n",
    "        kp6, d6 = orb_detector.detectAndCompute(img2_fwd, None)\n",
    "\n",
    "        # Match features between the two images. \n",
    "        # We create a Brute Force matcher with  \n",
    "        # Hamming distance as measurement mode. \n",
    "        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True) \n",
    "\n",
    "        # Match the two sets of descriptors. \n",
    "        matches = matcher.match(d1, d2) \n",
    "        matches_prev = matcher.match(d3, d4) \n",
    "        matches_fwd=matcher.match(d5, d6) \n",
    "        \n",
    "        if(max(len(matches),len(matches_prev),len(matches_fwd)) == len(matches)):\n",
    "            while(True):\n",
    "                # Convert to grayscale. \n",
    "                img2_color=ref_data[ref_ptr] \n",
    "                img2 = cv2.cvtColor(img2_color, cv2.COLOR_BGR2GRAY) \n",
    "                img2_prev = None\n",
    "                img2_fwd = None\n",
    "                if(ref_ptr>1):\n",
    "                    img2_prev = cv2.cvtColor(ref_data[ref_ptr-1], cv2.COLOR_BGR2GRAY)\n",
    "                else:\n",
    "                    img2_prev = cv2.cvtColor(ref_data[ref_ptr], cv2.COLOR_BGR2GRAY)\n",
    "                if(ref_ptr<len(ref_data)-1):\n",
    "                    img2_fwd = cv2.cvtColor(ref_data[ref_ptr+1], cv2.COLOR_BGR2GRAY)\n",
    "                else:\n",
    "                    img2_fwd = cv2.cvtColor(ref_data[ref_ptr], cv2.COLOR_BGR2GRAY)\n",
    "                height, width = img2.shape \n",
    "\n",
    "                # Create ORB detector with 5000 features. \n",
    "                orb_detector = cv2.ORB_create(5000) \n",
    "                orb_detector_prev = cv2.ORB_create(5000)\n",
    "                orb_detector_fwd = cv2.ORB_create(5000)\n",
    "                # Find keypoints and descriptors. \n",
    "                # The first arg is the image, second arg is the mask \n",
    "                #  (which is not reqiured in this case). \n",
    "                kp1, d1 = orb_detector.detectAndCompute(img1, None) \n",
    "                kp2, d2 = orb_detector.detectAndCompute(img2, None) \n",
    "\n",
    "                kp3, d3 = orb_detector_prev.detectAndCompute(img1, None) \n",
    "                kp4, d4 = orb_detector_prev.detectAndCompute(img2_prev, None)\n",
    "\n",
    "                kp5, d5 = orb_detector.detectAndCompute(img1, None) \n",
    "                kp6, d6 = orb_detector.detectAndCompute(img2_fwd, None)\n",
    "\n",
    "                # Match features between the two images. \n",
    "                # We create a Brute Force matcher with  \n",
    "                # Hamming distance as measurement mode. \n",
    "                matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True) \n",
    "\n",
    "                # Match the two sets of descriptors. \n",
    "                matches = matcher.match(d1, d2) \n",
    "                matches_prev = matcher.match(d3, d4) \n",
    "                matches_fwd=matcher.match(d5, d6) \n",
    "\n",
    "                if(max(len(matches),len(matches_prev),len(matches_fwd)) == len(matches)):\n",
    "                    break\n",
    "                elif(max(len(matches),len(matches_prev),len(matches_fwd)) == len(matches_prev)):\n",
    "                    ref_ptr-=1\n",
    "                    continue\n",
    "                else:\n",
    "                    ref_ptr+=1\n",
    "                    continue\n",
    "            break\n",
    "        elif(max(len(matches),len(matches_prev),len(matches_fwd)) == len(matches_prev)):\n",
    "            ref_ptr-=5\n",
    "            continue\n",
    "        else:\n",
    "            ref_ptr+=5\n",
    "            continue\n",
    "    # Sort matches on the basis of their Hamming distance. \n",
    "    matches.sort(key = lambda x: x.distance) \n",
    "\n",
    "    # Take the top 90 % matches forward. \n",
    "    matches = matches[:int(len(matches)*90)] \n",
    "    no_of_matches = len(matches) \n",
    "\n",
    "    # Define empty matrices of shape no_of_matches * 2. \n",
    "    p1 = np.zeros((no_of_matches, 2)) \n",
    "    p2 = np.zeros((no_of_matches, 2)) \n",
    "\n",
    "    for i in range(len(matches)): \n",
    "      p1[i, :] = kp1[matches[i].queryIdx].pt \n",
    "      p2[i, :] = kp2[matches[i].trainIdx].pt \n",
    "\n",
    "    # Find the homography matrix. \n",
    "    homography, mask = cv2.findHomography(p1, p2, cv2.RANSAC) \n",
    "\n",
    "    # Use this matrix to transform the \n",
    "    # colored image wrt the reference image. \n",
    "    transformed_img = cv2.warpPerspective(img1_color, \n",
    "                        homography, (width, height)) \n",
    "\n",
    "    (score, diff) = compare_ssim(img2,cv2.cvtColor(transformed_img, cv2.COLOR_BGR2GRAY), full=True)\n",
    "    diff = (diff * 255).astype(\"uint8\")\n",
    "    thresh = cv2.threshold(diff, 0, 255,cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    #kernel = np.ones((5,5),np.uint8)\n",
    "    #erosion = cv2.erode(diff,kernel,iterations = 1)\n",
    "    #blur=cv2.GaussianBlur(thresh, (15,15),0)\n",
    "    #blur=cv2.medianBlur(thresh,15)\n",
    "    #kernel=np.zeros((5,5), np.uint8)\n",
    "    #erosion = cv2.dilate(blur, kernel, iterations=1)\n",
    "    \n",
    "    cv2.namedWindow(\"thresh\", cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(\"thresh\", thresh)\n",
    "    cv2.imwrite('/NMPS-CD/Road/Output1'+str(i)+'.jpg',cv2.cvtColor(thresh,cv2.COLOR_GRAY2RGB))\n",
    "    #out.write(cv2.cvtColor(thresh,cv2.COLOR_GRAY2RGB))\n",
    "    #cv2.namedWindow(\"blur\", cv2.WINDOW_NORMAL)\n",
    "    #cv2.imshow(\"blur\", blur)\n",
    "    #blurw.write(cv2.cvtColor(blur,cv2.COLOR_GRAY2RGB))\n",
    "    \n",
    "    #cv2.namedWindow(\"erosion\", cv2.WINDOW_NORMAL)\n",
    "    #cv2.imshow(\"erosion\", erosion)\n",
    "    \n",
    "    #cv2.namedWindow(\"ref.jpg\", cv2.WINDOW_NORMAL)\n",
    "    #cv2.namedWindow(\"align.jpg\", cv2.WINDOW_NORMAL)\n",
    "    #cv2.imshow('ref.jpg', img2_color)\n",
    "    #cv2.imshow('align.jpg', img1_color)\n",
    "    keyboard = cv2.waitKey(30) & 0xFF\n",
    "    if keyboard == 'q' or keyboard == 27:\n",
    "        break\n",
    "#out.release()\n",
    "#blurw.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
